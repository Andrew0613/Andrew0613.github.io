<!DOCTYPE html>
<html>
  <head>
    <title>Ungrounded Meaning – Yuandong Pu – 蒲沅东</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Thoughts on paper Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand? and beyond.

" />
    <meta property="og:description" content="Thoughts on paper Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand? and beyond.

" />
    
    <meta name="author" content="Yuandong Pu" />

    
    <meta property="og:title" content="Ungrounded Meaning" />
    <meta property="twitter:title" content="Ungrounded Meaning" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Yuandong Pu - 蒲沅东" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <!--<a href="/" class="site-avatar"><img src="https://avatars0.githubusercontent.com/u/4877252?s=460&u=b7267190dec8cdb2c160b5aeae14320c95badd02&v=4" /></a> -->

          <div class="site-info">
            <h1 class="site-name">
              <a href="/">Yuandong Pu</a>
              <!--<small> <small> <small> 
<a href="mailto:puyuandong01061313@gmail.com"><i class="svg-icon email"></i></a>



<a href="https://scholar.google.com/citations?user=m-qhWXwAAAAJ"><i class="svg-icon googlescholar"></i></a>
<a href="https://github.com/Andrew0613"><i class="svg-icon github"></i></a>




<a href="https://www.twitter.com/AndrewDominic13"><i class="svg-icon twitter"></i></a>



 </small> </small> </small>-->
            </h1>
            <p class="site-description">蒲沅东</p>
          </div>

          <!--<nav>
            <a href="/">About</a>
            <a href="/blog">Blog</a>
            <a href="/papers">Papers</a>
          </nav> -->
          <nav>
            
<a href="mailto:puyuandong01061313@gmail.com"><i class="svg-icon email"></i></a>



<a href="https://scholar.google.com/citations?user=m-qhWXwAAAAJ"><i class="svg-icon googlescholar"></i></a>
<a href="https://github.com/Andrew0613"><i class="svg-icon github"></i></a>




<a href="https://www.twitter.com/AndrewDominic13"><i class="svg-icon twitter"></i></a>




          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Ungrounded Meaning</h1>

  <div class="entry">
    <p>Thoughts on paper <a href="https://arxiv.org/abs/2104.10809">Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?</a> and beyond.</p>

<h2 id="the-question">The Question</h2>

<p>Can language <strong>meaning</strong> be learned from <strong>form</strong> alone?</p>

<p>This is arguably the philosophial question most relevant to today’s NLP, asking if its data-driven paradigm is fundamentally sound. It sure feels politically correct to <a href="https://www.aclweb.org/anthology/2020.acl-main.463.pdf">say no</a>, but the detailed arguments and counter-arguments have produced a <a href="https://blog.julianmichael.org/2020/07/23/to-dissect-an-octopus.html">great debate</a>.</p>

<p>My personal take:</p>

<ul>
  <li>Representations from learning on corpus (word2vec, BERT contextual embeddings) are obviously <strong>meaningful</strong>. I still remember the shock many years ago seeing <code class="language-plaintext highlighter-rouge">king - man + woman = queen</code>.</li>
  <li>But whether such models have learned <strong>meaning</strong> require <strong>probing tasks</strong> that specify how a model shall behave if it learns language meaning. On many tasks (dialogue, storytelling, …) current NLP models perform poorly, and even huger corpus doesn’t seem a path to solving these.</li>
  <li>The tricky thing about probing tasks is that, your model will always get grounded when fine-tuned - you gain inductive bias about <strong>task intent</strong> through new data or new optimization or new parameters. In a sense, a toehold of grounding (or inductive bias) bootstraps the learning of meaning (and pre-training might make learning much faster and easier). But no matter how much power you gain from pre-training on form alone, you just can’t jump without a toehold of ground to jump from!
    <ul>
      <li>The empirical question is <strong>how large the toehold should be</strong>, so that empirical numbers make most sense.</li>
      <li>The theoretical question is: <strong>how small the toehold could be</strong>, so that learning meaning can be bootstrapped?</li>
    </ul>
  </li>
</ul>

<p><a href="https://arxiv.org/abs/2104.10809">Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?</a> is an interesting step toward the theoretical question. It argues meaning cannot be learned even with a <em>seemingly huge toehold</em>: the ability to query oracle if two texts have the same (contextual) meaning.</p>

<h2 id="the-setup">The Setup</h2>

<p>You can’t learn a Python compiler from just seeing Python code, since you don’t get to obserse any input/output execution results. But what if code is equipped with assertions?</p>

<pre><code class="language-Python">a = 3 + 5
assert a == 8
</code></pre>

<p>You may learn some meaning from it!</p>

<p>To be even more generuous, assume assertions are on your side. You get to choose two strings <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> to query, and assertion <code class="language-plaintext highlighter-rouge">assert x == y</code> tell you if they evaluate to the same value within their respective context.</p>

<p>For example, let <code class="language-plaintext highlighter-rouge">x</code> be</p>

<pre><code class="language-Python">a += 5
</code></pre>

<p>and let <code class="language-plaintext highlighter-rouge">y</code> be</p>

<pre><code class="language-Python">a += 2; if True: a += 3
</code></pre>

<p>They should be equivalent no matter what code precedes them, therefore they should have the same representation.</p>

<p><strong>The task is, assign each string <code class="language-plaintext highlighter-rouge">x</code> a representation <code class="language-plaintext highlighter-rouge">f(x)</code> , so that for any two strings, <code class="language-plaintext highlighter-rouge">f(x)=f(y)</code> iff <code class="language-plaintext highlighter-rouge">assert x==y</code> holds for all (valid) contexts.</strong></p>

<p>A projection back to the meaning-probing-grounding framework:</p>

<ul>
  <li>Added probe data? <strong>Unlimited</strong>, as long as you can process in computable time. Or equivalently, your original corpus is designed best for you and you have zero new data.</li>
  <li>Added probe power? <strong>Unlimited</strong>, in the sense that probe family is <em>any function</em> and you can choose the best function out of any function. Or equivalently, there is not probe <em>learning</em>, just about <em>existance</em>.</li>
  <li>Added anything else? **Understand the meaning of word <code class="language-plaintext highlighter-rouge">assert </code> and the meaning of it combined with arbitrary statements **. That’s the only toehold from form to meaning in the setup.</li>
  <li><em>Also note task is kind of harsh, you need representations to respect contextual equivalence for all strings and all contexts.</em></li>
</ul>

<h2 id="the-proof">The Proof</h2>

<p><img src="/images/main.png" alt="main" /></p>

<p>The idea of the proof is quite simple. Consider Python programs looking like either left or right, where <code class="language-plaintext highlighter-rouge">m</code> and <code class="language-plaintext highlighter-rouge">n</code> can be seen as integer constants. Here <code class="language-plaintext highlighter-rouge">tm_run</code> is essentially an Universal Turing Machine that takes Turing machine state <code class="language-plaintext highlighter-rouge">m</code> and returns Turing machine state <code class="language-plaintext highlighter-rouge">n</code> steps later.</p>

<ul>
  <li><strong>Oracle has no computability beyond Turing Machine</strong>, because for each concrete <code class="language-plaintext highlighter-rouge">n</code>, both programs can be run in finite time and compared.</li>
  <li><strong>“Emulating meaning” requires solving the halting problem (for every <code class="language-plaintext highlighter-rouge">m</code>) and is not computable</strong>, because for a fixed <code class="language-plaintext highlighter-rouge">m</code>, if</li>
</ul>

<p><img src="/images/decide.png" alt="decide" /></p>

<p>holds, it means it holds for all <code class="language-plaintext highlighter-rouge">n</code>, i.e. Turing Machine <code class="language-plaintext highlighter-rouge">m</code> does not halt. You simply can’t do that if your oracle isn’t beyond Turing Machine.</p>

<h2 id="back-to-the-setup">Back to The Setup</h2>

<p>I feel the main proof is rather tricky than insightful, and here are a few comments:</p>

<ul>
  <li>
    <p>Humans also cannot “emulate meaning” in such a setup, since we can’t solve the halting problem either?</p>
  </li>
  <li>
    <p>The considered language is too weak - just a tiny subset of possible Python programs, so that it makes oracle weaker (within TM) and task easier (but still beyond TM).</p>
  </li>
  <li>
    <p>Power about language is you can talk about language itself, and that’s the essence of Turing Machine. In a “complete” language, from assertion</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Program m cannot halt in any finite steps
</code></pre></div>    </div>

    <p>you can gain meaning about program <code class="language-plaintext highlighter-rouge">m</code>. This is because you understand meanings of extra things like “any” or “finite”. <strong>So perhaps minimal grounding requires knowing meaning beyond <code class="language-plaintext highlighter-rouge">assert</code> - maybe some other elements like <code class="language-plaintext highlighter-rouge">or</code> <code class="language-plaintext highlighter-rouge">any</code> <code class="language-plaintext highlighter-rouge">if</code>……</strong></p>
  </li>
</ul>

<h2 id="back-to-the-question">Back to The Question</h2>

<ul>
  <li>Theoretical question is hard, not yet well-defined. This paper seems to brings more question than answers.
    <ul>
      <li>Better setup or definition for “learning meaning” and “grounding” might be needed.</li>
      <li>Especially, in language of logic, grounding and inductive bias are very similar, and are very tricky.</li>
    </ul>
  </li>
  <li>Theoretical question might not be related to the empirical question (yet), especially when the theoretical question leads to negative answers.
    <ul>
      <li>Theory may not have been well-established, and setups are far from practice.</li>
      <li>Even if the theory is well-established, think of worse-case time complexity analysis vs. practicatily of some algorithm.</li>
    </ul>
  </li>
  <li>Key aspects about learning from form are still missing: modelling token co-occurance, for example, is not even touched in the paper. How co-occurance statistics lead to “structure” is still intriguing.</li>
</ul>


  </div>

  <div class="date">
    Written on April 24, 2021
  </div>

  
</article>

    </div>

    

  </body>
</html>
