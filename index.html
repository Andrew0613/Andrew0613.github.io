<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhiyuan You (Â∞§ÂøóËøú)</title>

  <meta name="author" content="Zhiyuan You">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhiyuan You (Â∞§ÂøóËøú)</name>
                  </p>
                  <p>I am a first-year Ph.D. student from <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>,
                    <a href="https://cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>,
                    supervised by Prof. <a href="https://tianfan.info/">Tianfan Xue</a> and Prof. <a
                      href="https://xpixel.group/people.html">Chao Dong</a>.
                    I received both my Bachelor's and Master's
                    degrees from <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, supervised
                    by Prof. <a href="https://scholar.google.com/citations?user=MGZyMf4AAAAJ&hl=zh-CN">Xinyi Le</a>
                    and Prof. <a href="https://me.sjtu.edu.cn/teacher_directory1/zhengyu.html">Yu Zheng</a>.
                  </p>
                  <p>
                    I spent a wonderful time in <a href="https://horizon.cc/">Horizon Robotics</a> and
                    <a href="https://www.sensetime.com/en">SenseTime</a> as a research intern, mentored by
                    Dr. <a href="https://scholar.google.com/citations?user=21NZzksAAAAJ&hl=zh-CN">Yuelong Yu </a> and
                    Mr. <a href="https://scholar.google.com/citations?user=s5Z6X0cAAAAJ&hl=zh-CN">Kai Yang</a>,
                    respectively.
                  </p>
                  <p>
                    My current research interest mainly lies in <span class="highlight">multi-modality vision for
                      low-level vision</span>.
                  </p>
                  <p style="text-align:center">
                    <a href="data/CV_ZhiyuanYOU.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=YRnwvDwAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/zhiyuanyou/">Github</a>
                    <br>
                    <a href="mailto:zhiyuanyou@foxmail.com">Email</a>: zhiyuanyou [at] foxmail [dot] com
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhiyuanyou.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhiyuanyou.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My previous work mainly focused on <span class="highlight">anomaly detection, few-shot
                      learning, and industrial visual inspection</span>. (*: Equal Contribution, ‚Ä†:
                    Corresponding Author)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

            <tr>
              <td style="padding-left:20px">
                <subheading>Arxiv, preprint</subheading>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/depictqa.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.08962">
                  <papertitle>Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language
                    Models
                  </papertitle>
                </a>
                <br>
                <strong>Zhiyuan You*</strong>,
                Zheyuan Li*, Jinjin Gu*, Zhenfei Yin, Tianfan Xue‚Ä†, Chao Dong‚Ä†
                <br>
                <em>Arxiv</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2312.08962">paper</a>
                /
                <a href="https://depictqa.github.io/">project page</a>
                <p></p>
                <p>We introduce <em>DepictQA</em>, leveraging Multi-modal Large Language Models, allowing for detailed,
                  language-based, and human-like evaluation of image quality.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-left:20px">
                <subheading>Conference</subheading>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/uniad.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.03687">
                  <papertitle>A Unified Model for Multi-class Anomaly Detection
                  </papertitle>
                </a>
                <br>
                <strong>Zhiyuan You*</strong>,
                Lei Cui*, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le‚Ä†
                <br>
                <em>NeurIPS</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2206.03687">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/UniAD">code</a>
                <p></p>
                <p>We present <em>UniAD</em> that accomplishes anomaly detection for multiple classes with a unified
                  framework.
                </p>
              </td>
            </tr>

            <tr onmouseout="cagroup_stop()" onmouseover="cagroup_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/safecount.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function cagroup_start() {
                    document.getElementById('cagroup_image').style.opacity = "1";
                  }
                  function cagroup_stop() {
                    document.getElementById('cagroup_image').style.opacity = "0";
                  }
                  cagroup_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2201.08959">
                  <papertitle>Few-shot Object Counting with Similarity-Aware Feature Enhancement</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le‚Ä†
                <br>
                <em>WACV</em>, 2023 &nbsp <font color="red"><strong>(Oral)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2201.08959">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/SAFECount">code</a>
                /
                <a href="https://youtu.be/JzrCVyWujDY">video</a>
                <p></p>
                <p>
                  We propose a novel <em>SAFECount</em> block, equipped with a similarity comparison module and a
                  feature enhancement module for few-shot object counting.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/adtr.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2209.01816">
                  <papertitle>ADTR: Anomaly Detection Transformer with Feature Reconstruction</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Lei Cui, Yu Zheng, Xinyi Le‚Ä†
                <br>
                <em>ICONIP</em>, 2022 &nbsp <font color="red"><strong>(Oral)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2209.01816">paper</a>
                <p></p>
                <p>We propose <em>ADTR</em> to apply a transformer to reconstruct pre-trained
                  features for anomaly detection, and propose novel losses to extend <em>ADTR</em> to
                  anomaly-available case (both image-level & pixel-level labeled).</p>
              </td>
            </tr>

            <tr>
              <td style="padding-left:20px">
                <subheading>Journal</subheading>
              </td>
            </tr>

            <tr onmouseout="mssvt_stop()" onmouseover="mssvt_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/utrad.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function mssvt_start() {
                    document.getElementById('mssvt_image').style.opacity = "1";
                  }
                  function mssvt_stop() {
                    document.getElementById('mssvt_image').style.opacity = "0";
                  }
                  mssvt_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">
                  <papertitle>UTRAD: Anomaly detection and localization with u-transformer</papertitle>
                </a>
                <br>
                Liyang Chen,
                <strong>Zhiyuan You</strong>,
                Nian Zhang,
                Juntong Xi,
                Xinyi Le‚Ä†
                <br>
                <em>Neural Networks</em>, 2022
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">paper</a> /
                <a href="https://github.com/gordon-chenmo/UTRAD">code</a>
                <p></p>
                <p>We introduce <em>UTRAD</em>, a U-TRansformer based Anomaly Detection framework.
                </p>
              </td>
            </tr>

    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Education</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cuhk.png" , width="80">
        </td>
        <td width="75%" valign="center">
          Ph.D. Student in Information Engineering @ The Chinese University of Hong Kong
          <br>
          Aug. 2023 - Current
          <br>
          Advisor: Prof. <a href="https://tianfan.info/">Tianfan Xue</a> and Prof. <a
            href="https://xpixel.group/people.html">Chao Dong</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sjtu.png" , width="80">
        </td>
        <td width="75%" valign="center">
          M.Eng in Mechanical Engineering @ Shanghai Jiao Tong University
          <br>
          Sep. 2020 - Mar. 2023
          <br>
          Advisor: Prof. <a href="https://scholar.google.com/citations?user=MGZyMf4AAAAJ&hl=zh-CN">Xinyi Le</a>
          and Prof. <a href="https://me.sjtu.edu.cn/teacher_directory1/zhengyu.html">Yu Zheng</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sjtu.png" , width="80"></td>
        <td width="75%" valign="center">
          B.Eng in Mechanical Engineering @ Shanghai Jiao Tong University
          <br>
          Sep. 2016 - Jun. 2020
          <br>
          GPA: 3.78 / 4.0, Ranking: 5 / 148
        </td>
      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/horizon.png" , width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://horizon.cc/">Horizon Robotics</a>
              <br>
              Research Intern
              <br>
              Shanghai, China
              <br>
              Dec. 2022 - Mar. 2023
              <br>
              Mentor: Dr. <a href="https://scholar.google.com/citations?user=21NZzksAAAAJ&hl=zh-CN">Yuelong
                Yu </a>
              <br>
              Perception Algorithm for Autonomous Driving
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png" , width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://www.sensetime.com/en">SenseTime</a>
              <br>
              Research Intern
              <br>
              Shanghai, China
              <br>
              Dec. 2020 - Nov. 2022
              <br>
              Mentor: Mr. <a href="https://scholar.google.com/citations?user=s5Z6X0cAAAAJ&hl=zh-CN">Kai Yang</a>
              <br>
              Anomaly Detection & Few-Shot Learning
            </td>
          </tr>

        </tbody>
      </table>
      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template from <a href="https://github.com/jonbarron/jonbarron_website">JonBarron</a>
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      </td>
      </tr>
  </table>
</body>

</html>