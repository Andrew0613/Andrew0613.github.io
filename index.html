<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuandong Pu (Ëí≤Ê≤Ö‰∏ú)</title>

  <meta name="author" content="Yuandong Pu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:50%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yuandong Pu (Ëí≤Ê≤Ö‰∏ú)</name>
                  </p>
                  <p>I am a first-year Ph.D. student of the joint program between
                    <a href="https://www.shlab.org.cn">Shanghai AI Laboratory</a> and Shanghai Jiao Tong University
                    (<a href="https://www.sjtu.edu.cn">SJTU</a>).
                    I am supervised by Prof. <a href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a>.
                  </p>
                  <p>
                    Prior to that, I received my B.Eng. from Beijing Institute of Technology(<a href="https://www.bit.edu.cn">BIT</a>) in 2023.
                  </p>
                  <p>
                    My current research interest mainly lies in <span class="highlight">image generation, image editing and general low-level vision</span>.
                  </p>
                  <p style="text-align:center">
                    <!-- <a href="data/CV_ZhiyuanYOU.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=m-qhWXwAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/Andrew0613">Github</a>
                    <br>
                    <a href="mailto:puyuandong01061313@gmail.com">Email</a>: puyuandong01061313 [at] gmail [dot] com
                  </p>
                </td>
                <td style="padding:2.5%;width:60%;max-width:60%">
                  <a href="images/puyuandong.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/puyuandong.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                    2024-07: One paper was accepted to ACM MM 2024.
                  </p>

                  <p>
                    2024-07: One paper was accepted to ECCV 2024.
                  </p>

                  <p>
                    2023-09: I became a Ph.D student at XPixel in SJTU.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    *: Equal Contribution, ‚Ä†: Corresponding Author
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

            <tr>
              <td style="padding-left:20px">
                <subheading>arXiv, preprint</subheading>
              </td>
            </tr>
                <!-- <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2405.18842">paper</a>
                /
                <a href="https://depictqa.github.io">project page</a>
                /
                <a href="https://github.com/XPixelGroup/DepictQA">code</a>
                <p></p>
                <p>We introduce <em>DepictQA-Wild</em>, a multi-functional in-the-wild descriptive image quality
                  assessment model. -->
                </p>
              </td>
            </tr>

            <!-- <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/maskma.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.11846">
                  <papertitle>
                    MaskMA: Towards Zero-Shot Multi-Agent Decision Making with Mask-Based Collaborative Learning
                  </papertitle>
                </a>
                <br>
                Jie Liu*, Yinmin Zhang*, Chuming Li, <strong>Zhiyuan You</strong>, 
                Zhanhui Zhou, Chao Yang, Yaodong Yang‚Ä†, Yu Liu, Wanli Ouyang
                <br>
                <em>arXiv</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2310.11846">paper</a>
                <p></p>
                <p>We release <em>MaskMA</em>, a masked pretraining framework for multi-agent decision-making. 
                </p>
              </td>
            </tr> -->

            <tr>
              <td style="padding-left:20px">
                <br><subheading>Conference</subheading>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/genlv.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <!-- <a href="https://arxiv.org/abs/2405.18842"> -->
                <a>
                  <papertitle>Learning A Low-Level Vision Generalist via Visual Task Prompt</papertitle>
                </a>
                <br>
                Xiangyu Chen, Yihao Liu,
                <strong>Yuandong Pu</strong>, 
                Wenlong Zhang, Jiantao Zhou, Yu Qiao, Chao Dong‚Ä†
                <br>
                <em>ACM MM</em>, 2024
                <br>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/x-restormer.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2310.11881">
                  <papertitle>A Comparative Study of Image Restoration Networks for General Backbone Network Design
                  </papertitle>
                </a>
                <br>
                Xiangyu Chen*,
                Zheyuan Li*, <strong>Yuandong Pu*</strong>, Yihao Liu, Jiantao Zhou, Yu Qiao, Chao Dong‚Ä†
                <br>
                <em>ECCV</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2310.11881">paper</a>
                /
                <a href="https://github.com/Andrew0613/X-Restormer">code</a>
                <p></p>
                </p>
              </td>
            </tr>

            <!-- <tr onmouseout="cagroup_stop()" onmouseover="cagroup_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/safecount.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function cagroup_start() {
                    document.getElementById('cagroup_image').style.opacity = "1";
                  }
                  function cagroup_stop() {
                    document.getElementById('cagroup_image').style.opacity = "0";
                  }
                  cagroup_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2201.08959">
                  <papertitle>Few-shot Object Counting with Similarity-Aware Feature Enhancement</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le‚Ä†
                <br>
                <em>WACV</em>, 2023 &nbsp <font color="red"><strong>(Oral)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2201.08959">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/SAFECount">code</a>
                /
                <a href="https://youtu.be/JzrCVyWujDY">video</a>
                <p></p>
                <p>
                  We propose a novel <em>SAFECount</em> block, equipped with a similarity comparison module and a
                  feature enhancement module for few-shot object counting.
                </p>
              </td>
            </tr>

            <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/uniad.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function fhnet_start() {
                    document.getElementById('fhnet_image').style.opacity = "1";
                  }
                  function fhnet_stop() {
                    document.getElementById('fhnet_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.03687">
                  <papertitle>A Unified Model for Multi-class Anomaly Detection
                  </papertitle>
                </a>
                <br>
                <strong>Zhiyuan You*</strong>,
                Lei Cui*, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le‚Ä†
                <br>
                <em>NeurIPS</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2206.03687">paper</a>
                /
                <a href="https://github.com/zhiyuanyou/UniAD">code</a>
                <p></p>
                <p>We present <em>UniAD</em> that accomplishes anomaly detection for multiple classes with a unified
                  framework.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/adtr.jpg' style="width:100%">
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2209.01816">
                  <papertitle>ADTR: Anomaly Detection Transformer with Feature Reconstruction</papertitle>
                </a>
                <br>
                <strong>Zhiyuan You</strong>,
                Kai Yang, Wenhan Luo, Lei Cui, Yu Zheng, Xinyi Le‚Ä†
                <br>
                <em>ICONIP</em>, 2022 &nbsp <font color="red"><strong>(Oral)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2209.01816">paper</a>
                <p></p>
                <p>We propose <em>ADTR</em> to apply a transformer to reconstruct pre-trained
                  features for anomaly detection, and propose novel losses to extend <em>ADTR</em> to
                  anomaly-available case (both image-level & pixel-level labeled).</p>
              </td>
            </tr> -->

            <tr>
              <td style="padding-left:20px">
                <br><subheading>Journal</subheading>
              </td>
            </tr>

            <!-- <tr onmouseout="mssvt_stop()" onmouseover="mssvt_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <img src='images/utrad.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function mssvt_start() {
                    document.getElementById('mssvt_image').style.opacity = "1";
                  }
                  function mssvt_stop() {
                    document.getElementById('mssvt_image').style.opacity = "0";
                  }
                  mssvt_stop()
                </script>
              </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">
                  <papertitle>UTRAD: Anomaly detection and localization with u-transformer</papertitle>
                </a>
                <br>
                Liyang Chen,
                <strong>Zhiyuan You</strong>,
                Nian Zhang,
                Juntong Xi,
                Xinyi Le‚Ä†
                <br>
                <em>Neural Networks</em>, 2022
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608021004810">paper</a> /
                <a href="https://github.com/gordon-chenmo/UTRAD">code</a>
                <p></p>
                <p>We introduce <em>UTRAD</em>, a U-TRansformer based Anomaly Detection framework.
                </p>
              </td>
            </tr> -->

    </tbody>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Education</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sjtu.png" , width="80">
        </td>
        <td width="75%" valign="center">
          Ph.D. Student in Electronic Engineering @ Shanghai Jiao Tong University
          <br>
          Sep. 2023 - Current
          <br>
          Advisor: Prof. <a href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Bit.png" , width="80"></td>
        <td width="75%" valign="center">
          B.Eng. in Artificial Intelligence @ Beijing Institute of Technology
          <br>
          Sep. 2019 - Jun. 2023
          <br>
          GPA: 3.8 / 4.0, <strong>Ranking: 1 / 79<strong>
        </td>
      </tr>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/horizon.png" , width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://horizon.cc/">Horizon Robotics</a>
              <br>
              Research Intern
              <br>
              Shanghai, China
              <br>
              Dec. 2022 - Mar. 2023
              <br>
              Mentor: Dr. <a href="https://scholar.google.com/citations?user=21NZzksAAAAJ">Yuelong
                Yu </a>
              <br>
              Perception Algorithm for Autonomous Driving
            </td>
          </tr>

        </tbody>
      </table> -->

      <table style="width:33%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=OxYdLLU39e4Y8ZbkKu8gqFRyz5lt4OjAbl7gCMlgO78&cl=ffffff&w=a"></script>
        </tbody>
      </table>

      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template from <a href="https://github.com/jonbarron/jonbarron_website">JonBarron</a>
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      </td>
      </tr>
  </table>
</body>

</html>